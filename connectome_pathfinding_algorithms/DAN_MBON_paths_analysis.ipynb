{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import navis\n",
    "import navis.interfaces.neuprint as neu\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "NC = neu.NeuronCriteria\n",
    "client = neu.Client('https://neuprint.janelia.org',\n",
    "                    token='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImphY2t3bGluZHNleUBnbWFpbC5jb20iLCJsZXZlbCI6Im5vYXV0aCIsImltYWdlLXVybCI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBVFhBSnhPaGpOWFpQZFdOc1cwdVJpLWtkcDNkR0Vfa0R1Z2pHVlVuaWFKPXM5Ni1jP3N6PTUwP3N6PTUwIiwiZXhwIjoxODE5MzczNTU3fQ.eRZd9IomfWpk6TSvop_h7i79YIXKJcgwa4b09CNfk2Y',\n",
    "                    dataset='hemibrain:v1.1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_meta1, roi1 = neu.fetch_neurons(NC(min_post=1))\n",
    "all_meta2, roi2 = neu.fetch_neurons(NC(min_pre=1))\n",
    "\n",
    "# Combine above dataframes\n",
    "all_roi = pd.concat([roi1, roi2], axis=0).drop_duplicates(['bodyId', 'roi'])\n",
    "meta = pd.concat([all_meta1, all_meta2], axis=0).drop_duplicates('bodyId')\n",
    "\n",
    "all_roi.to_csv(\"all_roi.csv\")\n",
    "meta.to_csv(\"meta.csv\")\n",
    "print(\"Number of neurons: \", meta.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = '/Users/jacklindsey/MBEM_navis/edges'\n",
    "has_type = ~meta['type'].isnull()\n",
    "is_traced =  meta.statusLabel.isin(['Roughly traced', 'Traced'])\n",
    "criteria = NC(bodyId=meta[has_type | is_traced].bodyId)\n",
    "\n",
    "print(\"Number of traced neurons: \", meta[has_type | is_traced].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f335540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download connectivity data\n",
    "#_, edges = neu.fetch_adjacencies(criteria, criteria, include_nonprimary=False, export_dir=export, batch_size=200)\n",
    "\n",
    "# Uncomment to load connectivity data from file\n",
    "edges = pd.read_csv(f'{os.path.expanduser(export)}/total-connections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ccc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of synapses: \", edges.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#synaptic inputs per neuron\n",
    "num_inputs = edges.groupby('bodyId_post').weight.sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "#normalized synaptic strengths -- fraction of input synapses\n",
    "edges['weight_normalized'] = edges.weight / edges.bodyId_post.map(num_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(edges[['bodyId_pre', 'bodyId_post']].values.flatten()).shape[0], 'unique neurons')\n",
    "print(edges.shape[0], 'connections')\n",
    "print(edges.weight.sum(), 'synapses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pam = meta.type.str.startswith('PAM', na='').astype(bool)\n",
    "is_ppl = meta.type.str.startswith('PPL', na='').astype(bool)\n",
    "\n",
    "dans = meta[is_pam | is_ppl]\n",
    "\n",
    "is_mbon = meta.type.str.startswith('MBON', na='').astype(bool)\n",
    "mbons = meta[is_mbon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_neurons = np.unique(edges[['bodyId_pre', 'bodyId_post']].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to save out files needed for Matrix_alg code\n",
    "'''\n",
    "unique_neurons = np.unique(edges[['bodyId_pre', 'bodyId_post']].values.flatten())\n",
    "N = len(unique_neurons)\n",
    "W = np.zeros([N, N])\n",
    "for e in range(len(edges)):\n",
    "    #print(e, len(edges))\n",
    "    n1 = np.where(unique_neurons==edges.bodyId_pre.values[e])[0][0]\n",
    "    n2 = np.where(unique_neurons==edges.bodyId_post.values[e])[0][0]\n",
    "    W[n1, n2] = edges.weight_normalized.values[e]\n",
    "    \n",
    "np.save(\"W.npy\", W)\n",
    "np.save(\"unique_neurons.npy\", unique_neurons)\n",
    "np.save(\"mbon_ids.npy\", mbons.bodyId.values)\n",
    "np.save(\"dan_ids.npy\", dans.bodyId.values)\n",
    "unique_types = []\n",
    "for n in unique_neurons:\n",
    "    unique_types.append((meta.type.values[np.where(meta.bodyId.values==n)[0]][0]))\n",
    "np.save(\"unique_neurons_types.npy\", unique_types)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Subsample graphs with specified number of nodes for scaling analyses\n",
    "N_options = np.logspace(8, 12, 5, base=2).astype(int)\n",
    "\n",
    "edge_subsets = []\n",
    "\n",
    "for N in N_options:\n",
    "    \n",
    "    node_subset = list(np.random.choice(list(set(np.concatenate([edges['bodyId_pre'], edges['bodyId_post']], 0))), size=(N,), replace=False))\n",
    "    node_subset.extend(mbons.bodyId)\n",
    "    node_subset.extend(dans.bodyId)\n",
    "    \n",
    "    edge_mask = np.logical_and(np.isin(edges['bodyId_pre'].values, node_subset), np.isin(edges['bodyId_post'].values, node_subset))\n",
    "    edge_subset = edges[edge_mask]\n",
    "    num_inputs = edge_subset.groupby('bodyId_post').weight.sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "    edge_subset['weight_normalized'] = edge_subset.weight / edge_subset.bodyId_post.map(num_inputs)\n",
    "    print(len(edge_subset))\n",
    "    \n",
    "    edge_subsets.append(edge_subset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling analysis -- runtime of sampling algorithm as function of number of nodes in graph\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sampling_utils\n",
    "import importlib\n",
    "importlib.reload(sampling_utils)\n",
    "import time\n",
    "\n",
    "n_proc_options = [1, 2, 4, 8]\n",
    "\n",
    "time_results = np.zeros([len(edge_subsets), len(n_proc_options)])\n",
    "ii = -1\n",
    "for edge_subset in edge_subsets:\n",
    "    ii += 1\n",
    "    jj = -1\n",
    "    \n",
    "    #vary number of processes in parallel implementation\n",
    "    for n_proc in n_proc_options:\n",
    "        jj += 1\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        model_edges_dan = edge_subset[['bodyId_pre', 'bodyId_post', 'weight_normalized']].rename({'bodyId_pre': 'target',\n",
    "                                                                                   'bodyId_post': 'source'}, axis=1)\n",
    "\n",
    "\n",
    "        paths_model_dan = sampling_utils.TraversalModel(edges=model_edges_dan,\n",
    "                             seeds=dans.bodyId.values,\n",
    "                             terminals=mbons.bodyId.values,\n",
    "                             weights='weight_normalized',\n",
    "                             max_steps=10)\n",
    "\n",
    "        paths_model_dan.run_parallel(iterations=1000, n_cores=n_proc)\n",
    "        end_time = time.monotonic()\n",
    "        total_time = end_time - start_time\n",
    "        print(ii, jj, total_time)\n",
    "        time_results[ii, jj] = total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a57edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling analysis plotting code\n",
    "\n",
    "for n in range(len(n_proc_options)):\n",
    "    plt.plot(np.log2(time_results[:, n]))\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(range(len(N_options)), N_options, fontsize=14)\n",
    "plt.xlabel(\"Graph size\\n(# nodes)\", fontsize=14)\n",
    "yticks = range(3, 8)\n",
    "plt.yticks(yticks, np.power(2, yticks).astype(int), fontsize=14)\n",
    "plt.ylabel(\"Runtime (s)\", fontsize=14)\n",
    "plt.legend(n_proc_options, title=\"# processes\", title_fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ScalingFig1.pdf\")\n",
    "plt.show()\n",
    "\n",
    "for N_idx in range(len(N_options)):\n",
    "    plt.plot(np.log2(time_results[N_idx, :]))\n",
    "\n",
    "plt.xticks(range(len(n_proc_options)), n_proc_options, fontsize=14)\n",
    "plt.xlabel(\"Number of processes\", fontsize=14)\n",
    "yticks = range(3, 8)\n",
    "plt.yticks(yticks, np.power(2, yticks).astype(int), fontsize=14)\n",
    "plt.ylabel(\"Runtime (s)\", fontsize=14)\n",
    "plt.legend(N_options, title=\"Graph size\\n(# nodes)\", title_fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ScalingFig2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run sampling algorithm on full graph\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sampling_utils\n",
    "import importlib\n",
    "importlib.reload(sampling_utils)\n",
    "\n",
    "\n",
    "model_edges_dan = edges[['bodyId_pre', 'bodyId_post', 'weight_normalized']].rename({'bodyId_pre': 'target',\n",
    "                                                                               'bodyId_post': 'source'}, axis=1)\n",
    "\n",
    "\n",
    "paths_model_dan = sampling_utils.TraversalModel(edges=model_edges_dan,\n",
    "                     seeds=dans.bodyId.values,\n",
    "                     terminals=mbons.bodyId.values,\n",
    "                     weights='weight_normalized',\n",
    "                     max_steps=10)\n",
    "\n",
    "# Run model with 1000 iterations using num processors = num cores\n",
    "paths_model_dan.run_parallel(iterations=1000, n_cores=os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b582b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k_max = 10\n",
    "lengths = np.array([len(res) for res in paths_model_dan.results])\n",
    "\n",
    "fractional_contributions = [np.mean(lengths==k) for k in range(k_max+1)]\n",
    "for k in range(2, k_max+1):\n",
    "    plt.bar(k, fractional_contributions[k], color=\"tab:blue\")\n",
    "plt.xticks(range(2, k_max+1), np.arange(1, k_max))\n",
    "plt.xlabel(\"Number of steps\", fontsize=16)\n",
    "plt.ylabel(\"Fractional contribution\\nto interaction\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Sampling_alg_steps_contribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for aggregating contribution of different cell types to paths of different lengths k between sources and targets\n",
    "\n",
    "import re\n",
    "type_count = {}\n",
    "for k in range(11):\n",
    "    type_count[k] = []\n",
    "    \n",
    "type_count_overall = []\n",
    "for path in paths_model_dan.results:\n",
    "    path_length = len(path)\n",
    "    for node in path[1:-1]:\n",
    "        typ = meta.type.values[np.where(meta.bodyId.values==node)[0]][0]\n",
    "        if typ is None:\n",
    "            continue\n",
    "        coarse_typ = \"\"\n",
    "        for t in typ:\n",
    "            if t.isalpha() and t.isupper():\n",
    "                coarse_typ = coarse_typ + t\n",
    "        type_count[path_length].append(coarse_typ)\n",
    "        \n",
    "        type_count_overall.append(coarse_typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot centrality analysis\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "k = 3\n",
    "counts = OrderedDict(Counter(type_count_overall).most_common())\n",
    "df = pd.DataFrame.from_dict(counts, orient='index')\n",
    "df[0] = df[0].values / np.sum(df[0].values)\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "df = df[:20]\n",
    "df.plot(kind='bar', figsize=(10, 5), legend=False)\n",
    "\n",
    "plt.ylabel(\"Centrality\", fontsize=16)\n",
    "plt.xlabel(\"Cell type\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Centrality_fig_sampling_alg.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
