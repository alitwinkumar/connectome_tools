{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must be saved out by running the code in DAN_MBON_paths_analysis.ipynb\n",
    "\n",
    "W = np.load(\"W.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_neurons = list(np.load(\"unique_neurons.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [unique_neurons.index(idx) for idx in np.load(\"mbon_ids.npy\")]\n",
    "targets = [unique_neurons.index(idx) for idx in np.load(\"dan_ids.npy\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avoid paths that loop back to source nodes\n",
    "for s in sources:\n",
    "    W[:, s] = 0\n",
    "    \n",
    "#for t in targets:\n",
    "#    W[t, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling of most computationally intensive matrix multiplication step in the vectorized centrality algorithm\n",
    "\n",
    "\n",
    "\n",
    "N_options = [2**10, 2**11, 2**12, 2**13, 2**14] #Subsampled graph sizes\n",
    "cpu_times = np.zeros([len(N_options)])\n",
    "cpu_noparallel_times = np.zeros([len(N_options)])\n",
    "gpu_times = np.zeros([len(N_options)])\n",
    "\n",
    "for mode in [\"gpu\", \"cpu\", \"cpu_noparallel\"]:\n",
    "    for N in N_options:\n",
    "        W_sub = W[:N, :N] #Subsampling step to test scaling with graph size\n",
    "        print(mode, N)\n",
    "    \n",
    "        if mode == \"gpu\":\n",
    "            W_cuda = torch.Tensor(W_sub).cuda()\n",
    "        elif mode == \"cpu\":\n",
    "            W_cuda = torch.Tensor(W_sub).cpu()\n",
    "        elif mode == \"cpu_noparallel\":\n",
    "            W_cuda = np.array(W_sub)\n",
    "            \n",
    "        \n",
    "\n",
    "        start_time = time.monotonic()\n",
    "        k_max = 9 #max path length\n",
    "        W_effective_list = [W_sub]\n",
    "        current = W_cuda\n",
    "        if mode == \"cpu_noparallel\":\n",
    "            current = np.copy(W_sub)\n",
    "        for k in range(k_max):\n",
    "            if mode == \"cpu_noparallel\":\n",
    "                current = np.matmul(current, W_sub)\n",
    "                W_effective_list.append(np.copy(current))\n",
    "            else:\n",
    "                current = torch.matmul(current, W_cuda)\n",
    "                W_effective_list.append(current.cpu().detach().numpy())\n",
    "\n",
    "        end_time = time.monotonic()\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "        if mode == \"gpu\":\n",
    "            gpu_times[N_options.index(N)] = total_time\n",
    "        elif mode == \"cpu\":\n",
    "            cpu_times[N_options.index(N)] = total_time\n",
    "        elif mode == \"cpu_noparallel\":\n",
    "            cpu_noparallel_times[N_options.index(N)] = total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot scaling analysis\n",
    "\n",
    "plt.plot(np.log2(gpu_times))\n",
    "plt.plot(np.log2(cpu_times))\n",
    "plt.plot(np.log2(cpu_noparallel_times))\n",
    "yticks = np.arange(-4, 7, 2)\n",
    "plt.yticks(yticks, np.power(2.0, yticks), fontsize=14)\n",
    "plt.xticks(range(len(N_options)), N_options, fontsize=14)\n",
    "\n",
    "plt.legend([\"GPU\", \"CPU (parallel)\", \"CPU (serial)\"], fontsize=14)\n",
    "\n",
    "plt.ylabel(\"Running time (s)\", fontsize=16)\n",
    "plt.xlabel(\"Graph size (N)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Matrix_alg_scaling.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run vectorized centrality algorithm on full data\n",
    "\n",
    "W_cuda = torch.Tensor(W).cuda()\n",
    "\n",
    "k_max = 9\n",
    "W_effective_list = [W]\n",
    "current = W_cuda\n",
    "for k in range(k_max):\n",
    "    print(k)\n",
    "    current = torch.matmul(current, W_cuda)\n",
    "    W_effective_list.append(current.cpu().detach().numpy())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute (c^k_n)^(in) and (c^k_n)^(out)\n",
    "\n",
    "contribution_to_targets_list = []\n",
    "contribution_from_sources_list = []\n",
    "for k in range(k_max):\n",
    "    contribution_to_targets = W_effective_list[k][:, targets].sum(1)\n",
    "    contribution_from_sources = W_effective_list[k][sources].sum(0)\n",
    "    \n",
    "    contribution_to_targets_list.append(contribution_to_targets)\n",
    "    contribution_from_sources_list.append(contribution_from_sources)\n",
    "    \n",
    "#compute (c^k_n)\n",
    "contribution_list = []\n",
    "for k in range(3, k_max):\n",
    "    contribution = np.zeros([W.shape[0]])\n",
    "    for k_in in range(1, k-1):\n",
    "        contribution += (contribution_from_sources_list[k_in] * contribution_to_targets_list[k-k_in]) / k\n",
    "    contribution_list.append(contribution)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistic: total contribution of paths of different lengths\n",
    "\n",
    "totals = []\n",
    "for k in range(k_max):\n",
    "    totals.append(W_effective_list[k][sources][:, targets].sum())\n",
    "for k in range(k_max):\n",
    "    plt.bar(k, totals[k] / np.sum(totals), color=\"tab:blue\")\n",
    "    \n",
    "plt.xticks(range(k_max), np.arange(k_max)+1)\n",
    "plt.xlabel(\"Number of steps\", fontsize=16)\n",
    "plt.ylabel(\"Fractional contribution\\nto interaction\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Matrix_alg_steps_contribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_neurons_types = np.load(\"unique_neurons_types.npy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for aggregating contribution of different cell types to paths of different lengths k between sources and targets\n",
    "\n",
    "\n",
    "type_count = {}\n",
    "for k in range(k_max):\n",
    "    type_count[k] = {}\n",
    "type_count_overall = {}\n",
    "for k in range(k_max-3):\n",
    "    for n in range(len(unique_neurons_types)):\n",
    "        typ = unique_neurons_types[n]\n",
    "        if typ is None:\n",
    "            continue\n",
    "        coarse_typ = \"\"\n",
    "        for t in typ:\n",
    "            if t.isalpha() and t.isupper():\n",
    "                coarse_typ = coarse_typ + t\n",
    "        if coarse_typ not in type_count[k+3].keys():\n",
    "            type_count[k+3][coarse_typ] = 0\n",
    "            \n",
    "        if coarse_typ not in type_count_overall.keys():\n",
    "            type_count_overall[coarse_typ] = 0\n",
    "            \n",
    "        \n",
    "        \n",
    "        type_count[k+3][coarse_typ] += contribution_list[k][n]\n",
    "        type_count_overall[coarse_typ] += contribution_list[k][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot centrality analysis\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "k = 3\n",
    "counts = type_count_overall\n",
    "df = pd.DataFrame.from_dict(counts, orient='index')\n",
    "df.sort_values(0, ascending=False, inplace=True)\n",
    "df[0] = df[0].values / np.sum(df[0].values)\n",
    "df = df[:20]\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "df.plot(kind='bar', figsize=(10, 5), legend=False)\n",
    "\n",
    "plt.ylabel(\"Centrality\", fontsize=16)\n",
    "plt.xlabel(\"Cell type\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Centrality_fig_matrix_alg.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
